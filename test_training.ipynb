{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run model training and validates performances on the validation data**\n",
    "### **Author:** Alessandro Ulivi (ale.ulivi@gmail.com)\n",
    "### **Start day (yyyy/mm/dd):** 2024/10/18\n",
    "### **Description**\n",
    "#### The notebook:\n",
    "#### - loads train and validation data from the pip2_segmentation dataset (refer to README.txt).\n",
    "#### - Sets up and applies augmentation transformations.\n",
    "#### - Sets up the hyperparameters used to define and train the UNet model.\n",
    "#### - Trains the model on the (augmented) train data while validating training performances on the validation data.\n",
    "#### - While training, saves checkpoints.\n",
    "#### - While training, logs in TensorBoard: the training loss function, the validation loss function and metric, input and output images for the train and validation data.\n",
    "#### - Logs in TensorBoard the hypeparameters used for training.\n",
    "\n",
    "### **Requirements**\n",
    "#### The notebook runs on the pip2_segmentation environment and using the scripts of the pip2_segmentation project. Refer to https://github.com/AlessandroUlivi/pip2_segmentation.\n",
    "#### In addition, a \"runs\" folder and a \"checkpoints\" folder are expected to store, respectively, TensorBoards summaries of individual runs, and checkpoints of model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tensorboard extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import RandomSampler, DataLoader, Subset\n",
    "from dataprep.data_preparation import make_dataset, compose, random_flip, random_translation, random_gaussian_or_uniform_noise, add_channel, normalize, to_tensor\n",
    "from utils.utils_funct import dict2mdtable\n",
    "from models.unet import UNet\n",
    "from modeltrain.train_model import run_training, run_training_no_val\n",
    "from metrics.metric import DiceCoefficient, DiceLoss, DiceBCELoss, FocalLoss, BCE_EdgeDiceLoss\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_data_dir = r\"C:\\Users\\aless\\OneDrive\\Desktop\\Ale\\personal\\projects\\pip2_segmentation\\data\\train\\raw\"\n",
    "train_label_data_dir = r\"C:\\Users\\aless\\OneDrive\\Desktop\\Ale\\personal\\projects\\pip2_segmentation\\data\\train\\label\"\n",
    "val_input_data_dir = r\"C:\\Users\\aless\\OneDrive\\Desktop\\Ale\\personal\\projects\\pip2_segmentation\\data\\validation\\raw\"\n",
    "val_label_data_dir = r\"C:\\Users\\aless\\OneDrive\\Desktop\\Ale\\personal\\projects\\pip2_segmentation\\data\\validation\\label\"\n",
    "test_input_data_dir = r\"C:\\Users\\aless\\OneDrive\\Desktop\\Ale\\personal\\projects\\pip2_segmentation\\data\\test\\raw\"\n",
    "test_label_data_dir = r\"C:\\Users\\aless\\OneDrive\\Desktop\\Ale\\personal\\projects\\pip2_segmentation\\data\\test\\label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate trasformations\n",
    "train_data_transformations_w_augmentation = [random_flip, random_translation, random_gaussian_or_uniform_noise, add_channel, to_tensor]\n",
    "train_trafos = partial(compose, transforms=train_data_transformations_w_augmentation)\n",
    "\n",
    "val_data_transformations = [add_channel, to_tensor] #NOTE: Data are not normalized as the normalization had been done at the moment of dataset creation and before chuking the images\n",
    "val_trafos = partial(compose, transforms=val_data_transformations)\n",
    "\n",
    "#create the train and validation datasets\n",
    "train_dataset = make_dataset(train_input_data_dir, train_label_data_dir, transform=train_trafos, shuffle_data=True, stack_axis=0)\n",
    "val_dataset = make_dataset(val_input_data_dir, val_label_data_dir, transform=val_trafos, shuffle_data=True, stack_axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 31768), started 3:30:27 ago. (Use '!kill 31768' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-806d2f252e0b8ef4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-806d2f252e0b8ef4\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# open tensorboard inside of our notebook\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========\n",
    "# # pass data to DataLoader\n",
    "batch_size = 4\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "#only work on a small subset of the data, for the moment\n",
    "num_train_samples = 12\n",
    "train_sample_ds = Subset(train_dataset, np.arange(num_train_samples))\n",
    "train_sample_sampler = RandomSampler(train_sample_ds)\n",
    "train_loader = DataLoader(train_sample_ds, sampler=train_sample_sampler, batch_size=batch_size)\n",
    "\n",
    "num_val_samples = 12\n",
    "val_sample_ds = Subset(val_dataset, np.arange(num_val_samples))\n",
    "val_sample_sampler = RandomSampler(val_sample_ds)\n",
    "val_loader = DataLoader(val_sample_ds, sampler=val_sample_sampler, batch_size=batch_size)\n",
    "\n",
    "#=========\n",
    "# pass to device\n",
    "# if torch.cuda.is_available:\n",
    "#     print(\"using gpu\")\n",
    "#     device = torch.device(\"cuda\")\n",
    "# else:\n",
    "#     print(\"using cpu\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#=========\n",
    "# set model's parameters\n",
    "final_activation=\"Sigmoid\"\n",
    "depth = 4\n",
    "num_fmaps = 64\n",
    "fmap_inc_factor = 4\n",
    "downsample_factor = 2\n",
    "kernel_size = 3\n",
    "padding = \"same\"\n",
    "upsample_mode = \"nearest\"\n",
    "unet_model = UNet(depth=depth,\n",
    "                  in_channels=1,\n",
    "                  out_channels=1,\n",
    "                  final_activation=final_activation,\n",
    "                  num_fmaps=num_fmaps,\n",
    "                  fmap_inc_factor=fmap_inc_factor,\n",
    "                  downsample_factor=downsample_factor,\n",
    "                  kernel_size=kernel_size,\n",
    "                  padding=padding,\n",
    "                  upsample_mode=upsample_mode).to(device)\n",
    "\n",
    "#=========\n",
    "# set loss function\n",
    "# loss_function = nn.BCELoss() #second place for the BCELoss - for the moment it seems that it does not manage to get values increasing... they remain low and the Sigmoids then fails\n",
    "# loss_function = DiceLoss() #Works for the very initial training but then quick leads to large \"positive pixels\" structures\n",
    "# loss_function = DiceBCELoss() #for the moment it seems that this is the best\n",
    "loss_function = BCE_EdgeDiceLoss()\n",
    "bce_weight = 1\n",
    "dice_weight = 1\n",
    "#it might be worthed to test FocalLoss (https://arxiv.org/pdf/1708.02002, https://www.kaggle.com/code/bigironsphere/loss-function-library-keras-pytorch)\n",
    "\n",
    "#=========\n",
    "# set optimizer\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(unet_model.parameters(), lr=lr)\n",
    "\n",
    "#=========\n",
    "# set metrics\n",
    "bin_threshold=0.5\n",
    "metric = DiceCoefficient()\n",
    "\n",
    "#=========\n",
    "# indicate key\n",
    "# runs_counter = get_var_value(filename=\"varstore.dat\")\n",
    "my_key  = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# set logger's parameters\n",
    "logger = SummaryWriter(f\"runs/{my_key}\")\n",
    "log_interval=1\n",
    "log_image_interval=20\n",
    "\n",
    "\n",
    "#unrelated comment, could be useful to look into this https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/12 (0%)]\tLoss: 0.765636\n",
      "Train Epoch: 0 [4/12 (33%)]\tLoss: 0.577950\n",
      "Train Epoch: 0 [8/12 (67%)]\tLoss: 0.615042\n",
      "\n",
      "Validate: Average loss: 0.9578, Average Metric: 0.0031\n",
      "\n",
      "Train Epoch: 1 [0/12 (0%)]\tLoss: 0.973031\n",
      "Train Epoch: 1 [4/12 (33%)]\tLoss: 0.569853\n",
      "Train Epoch: 1 [8/12 (67%)]\tLoss: 0.583349\n",
      "\n",
      "Validate: Average loss: 0.5149, Average Metric: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#=========\n",
    "# model's training with validation\n",
    "n_epochs = 2\n",
    "lr_scheduler_flag = True\n",
    "lr_kwargs={\"mode\":\"min\", \"factor\": 0.1, \"patience\":2}\n",
    "checkpoint_save_path = \"checkpoints\"\n",
    "run_training(model=unet_model,\n",
    "            optimizer=optimizer,\n",
    "            metric=metric, \n",
    "            n_epochs=n_epochs,\n",
    "            bce_weight=bce_weight,\n",
    "            dice_weight=dice_weight,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            loss_function=loss_function,\n",
    "            bin_threshold=bin_threshold,\n",
    "            logger=logger,\n",
    "            log_interval=log_interval,\n",
    "            log_image_interval=log_image_interval,\n",
    "            device=device,\n",
    "            key=my_key,\n",
    "            path=checkpoint_save_path,\n",
    "            lr_scheduler_flag=lr_scheduler_flag,\n",
    "            lr_kwargs=lr_kwargs,\n",
    "            x_dim=[-2,-1],\n",
    "            y_dim=[-2,-1],\n",
    "            best_metric_init=0)\n",
    "\n",
    "#=========\n",
    "#log all hyperparameters as text in Tensorboard\n",
    "\n",
    "#transform the kwards of the lr in a string\n",
    "lr_kwargs_str = \"\"\n",
    "for k in lr_kwargs:\n",
    "    lr_kwargs_str = lr_kwargs_str + f\"{k}:{lr_kwargs[k]},\"\n",
    "\n",
    "#form a dictionary to with all hyperparameters to be logged\n",
    "hparam_dict = {\"batch_size\":str(batch_size),\n",
    "                      \"final_activation\":final_activation,\n",
    "                      \"depth\":str(depth),\n",
    "                      \"num_fmaps\":str(num_fmaps),\n",
    "                      \"fmap_inc_factor\":str(fmap_inc_factor),\n",
    "                      \"downsample_factor\":str(downsample_factor),\n",
    "                      \"kernel_size\":str(kernel_size),\n",
    "                      \"padding\":padding,\n",
    "                      \"upsample_mode\":upsample_mode,\n",
    "                      \"loss_function\":str(loss_function),\n",
    "                      \"bce_weight\":str(bce_weight),\n",
    "                      \"dice_weight\":str(dice_weight),\n",
    "                      \"bin_threshold\":str(bin_threshold),\n",
    "                      \"optimizer\":str(optimizer),\n",
    "                      \"metric\":str(metric),\n",
    "                      \"n_epochs\":str(n_epochs),\n",
    "                      \"lr_scheduler_flag\":str(lr_scheduler_flag),\n",
    "                      \"lr_kwargs\":lr_kwargs_str}\n",
    "\n",
    "#transform the dictionary in a table-like string object\n",
    "hparam_table_like = dict2mdtable(hparam_dict, key='Name', val='Value', transform_2_string=False)\n",
    "\n",
    "#log the text in Tensorboard summary of the run\n",
    "logger.add_text('Hyperparams', hparam_table_like, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #=========\n",
    "# # model's training without validation\n",
    "# n_epochs = 8\n",
    "# lr_scheduler_flag = True\n",
    "# lr_kwargs={\"mode\":\"min\", \"factor\": 0.1, \"patience\":2}\n",
    "# checkpoint_save_path = \"checkpoints\"\n",
    "# run_training_no_val(model=unet_model,\n",
    "#                     optimizer=optimizer,\n",
    "#                     metric=metric,\n",
    "#                     n_epochs=n_epochs,\n",
    "#                     train_loader=train_loader,\n",
    "#                     loss_function=loss_function,\n",
    "#                     bin_threshold=bin_threshold,\n",
    "#                     logger=logger,\n",
    "#                     log_interval=log_interval,\n",
    "#                     log_image_interval=log_image_interval,\n",
    "#                     device=device,\n",
    "#                     key=my_key,\n",
    "#                     path=checkpoint_save_path,\n",
    "#                     lr_scheduler_flag = lr_scheduler_flag,\n",
    "#                     lr_kwargs=lr_kwargs,\n",
    "#                     x_dim=[-2,-1],\n",
    "#                     y_dim=[-2,-1],\n",
    "#                     best_metric_init = -1)\n",
    "\n",
    "# #=========\n",
    "# #log all hyperparameters as text in Tensorboard\n",
    "\n",
    "# #transform the kwards of the lr in a string\n",
    "# lr_kwargs_str = \"\"\n",
    "# for k in lr_kwargs:\n",
    "#     lr_kwargs_str = lr_kwargs_str + f\"{k}:{lr_kwargs[k]},\"\n",
    "\n",
    "# #form a dictionary to with all hyperparameters to be logged\n",
    "# hparam_dict = {\"batch_size\":str(batch_size),\n",
    "#                       \"final_activation\":final_activation,\n",
    "#                       \"depth\":str(depth),\n",
    "#                       \"num_fmaps\":str(num_fmaps),\n",
    "#                       \"fmap_inc_factor\":str(fmap_inc_factor),\n",
    "#                       \"downsample_factor\":str(downsample_factor),\n",
    "#                       \"kernel_size\":str(kernel_size),\n",
    "#                       \"padding\":padding,\n",
    "#                       \"upsample_mode\":upsample_mode,\n",
    "#                       \"loss_function\":str(loss_function),\n",
    "#                       \"bin_threshold\":str(bin_threshold),\n",
    "#                       \"optimizer\":str(optimizer),\n",
    "#                       \"metric\":str(metric),\n",
    "#                       \"n_epochs\":str(n_epochs),\n",
    "#                       \"lr_scheduler_flag\":str(lr_scheduler_flag),\n",
    "#                       \"lr_kwargs\":lr_kwargs_str}\n",
    "\n",
    "# #transform the dictionary in a table-like string object\n",
    "# hparam_table_like = dict2mdtable(hparam_dict, key='Name', val='Value', transform_2_string=False)\n",
    "\n",
    "# #log the text in Tensorboard summary of the run\n",
    "# logger.add_text('Hyperparams', hparam_table_like, 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip2_segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
