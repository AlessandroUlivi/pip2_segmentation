{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tests a trained model**\n",
    "### **Author:** Alessandro Ulivi (ale.ulivi@gmail.com)\n",
    "### **Start day (yyyy/mm/dd):** 2024/10/21\n",
    "### **Description**\n",
    "#### The notebook loads a model and tests it on a test set.\n",
    "\n",
    "### **Requirements**\n",
    "#### The notebook expects a folder named \"tests\" in the work directory, to save summary writer's outputs.\n",
    "#### The notebook runs on the pip2_segmentation environment and using the scripts of the pip2_segmentation project. Refer to https://github.com/AlessandroUlivi/pip2_segmentation.\n",
    "In addition, a \"runs\" folder and a \"checkpoints\" folder are expected to store, respectively, TensorBoards summaries of individual runs, and checkpoints of model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tensorboard extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required modules\n",
    "import datetime\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import RandomSampler, DataLoader, Subset\n",
    "from data_preparation import make_dataset, add_channel, to_tensor, compose\n",
    "from unet import UNet\n",
    "from utils import dict2mdtable, load_checkpoint\n",
    "from test_model import test_model\n",
    "from metric import DiceCoefficient, DiceLoss, DiceBCELoss\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# import torchvision.transforms.v2 as transforms_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_data_dir = r\"C:\\Users\\aless\\OneDrive\\Desktop\\Ale\\personal\\projects\\pip2_segmentation\\data\\test\\raw\"\n",
    "test_label_data_dir = r\"C:\\Users\\aless\\OneDrive\\Desktop\\Ale\\personal\\projects\\pip2_segmentation\\data\\test\\label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate trasformations - #NOTE: Data are not normalized as the normalization had been done at the moment of dataset creation and before chunking the images\n",
    "test_data_transformations = [add_channel, to_tensor]\n",
    "test_trafos = trafos = partial(compose, transforms=test_data_transformations)\n",
    "\n",
    "#create the test dataset\n",
    "test_dataset = make_dataset(test_input_data_dir, test_label_data_dir, transform=test_trafos, shuffle_data=True, stack_axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4b0c33d38ffab5c4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4b0c33d38ffab5c4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# open tensorboard inside of our notebook\n",
    "%tensorboard --logdir tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========\n",
    "# # pass data to DataLoader\n",
    "batch_size=1\n",
    "\n",
    "#only work on a small subset of the data, for the moment\n",
    "num_test_samples = 4\n",
    "test_sample_ds = Subset(test_dataset, np.arange(num_test_samples))\n",
    "test_sample_sampler = RandomSampler(test_sample_ds)\n",
    "test_loader = DataLoader(test_sample_ds, sampler=test_sample_sampler, batch_size=batch_size)\n",
    "\n",
    "#=========\n",
    "# pass to device\n",
    "# if torch.cuda.is_available:\n",
    "#     print(\"using gpu\")\n",
    "#     device = torch.device(\"cuda\")\n",
    "# else:\n",
    "#     print(\"using cpu\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#=========\n",
    "# set model's parameters\n",
    "final_activation=\"Sigmoid\"\n",
    "depth = 3\n",
    "num_fmaps = 64\n",
    "fmap_inc_factor = 4\n",
    "downsample_factor = 2\n",
    "kernel_size = 3\n",
    "padding = \"valid\"\n",
    "upsample_mode = \"nearest\"\n",
    "unet_model = UNet(depth=depth,\n",
    "                  in_channels=1,\n",
    "                  out_channels=1,\n",
    "                  final_activation=final_activation,\n",
    "                  num_fmaps=num_fmaps,\n",
    "                  fmap_inc_factor=fmap_inc_factor,\n",
    "                  downsample_factor=downsample_factor,\n",
    "                  kernel_size=kernel_size,\n",
    "                  padding=padding,\n",
    "                  upsample_mode=upsample_mode).to(device)\n",
    "\n",
    "#=========\n",
    "# set loss function\n",
    "# loss_function = nn.BCELoss() #second place for the BCELoss - for the moment it seems that it does not manage to get values increasing... they remain low and the Sigmoids then fails\n",
    "# loss_function = DiceLoss() #Works for the very initial training but then quick leads to large \"positive pixels\" structures\n",
    "loss_function = DiceBCELoss() #for the moment it seems that this is the best\n",
    "#it might be worthed to test FocalLoss (https://arxiv.org/pdf/1708.02002, https://www.kaggle.com/code/bigironsphere/loss-function-library-keras-pytorch)\n",
    "\n",
    "#=========\n",
    "# set optimizer\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(unet_model.parameters(), lr=lr)\n",
    "\n",
    "#=========\n",
    "# set metrics\n",
    "bin_threshold=0.5\n",
    "metric = DiceCoefficient()\n",
    "\n",
    "#=========\n",
    "# indicate key\n",
    "# runs_counter = get_var_value(filename=\"varstore.dat\")\n",
    "my_key  = \"test_\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# set logger's parameters\n",
    "logger = SummaryWriter(f\"tests/{my_key}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========\n",
    "#=========\n",
    "#=========\n",
    "#THIS PART OF THE CODE IS NOT PROPERLY TESTED. IN PARTICULAR, NOTHING IS PROVITED TO step IN test_model\n",
    "#=========\n",
    "#=========\n",
    "#=========\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#=========\n",
    "#load checkpoint of the model to test\n",
    "checkpoint_save_path = \"checkpoints\"\n",
    "checkpoint_key = \"20241113-2343746\"\n",
    "model, optimizer, epoch = load_checkpoint(model=unet_model,\n",
    "                                          path=checkpoint_save_path,\n",
    "                                          optimizer=optimizer,\n",
    "                                          key=checkpoint_key)\n",
    "\n",
    "\n",
    "#=========\n",
    "# model's training without validation\n",
    "avg_loss_val, avg_metric_val = test_model(model=model,\n",
    "                                          loader=test_loader,\n",
    "                                          loss_function=loss_function,\n",
    "                                          metric=metric,\n",
    "                                          bin_threshold=bin_threshold,\n",
    "                                          step=None,\n",
    "                                          tb_logger=None,\n",
    "                                          device=None,\n",
    "                                          x_dim=[-2,-1],\n",
    "                                          y_dim=[-2,-1])\n",
    "\n",
    "#log avg_loss_val, avg_metric_val to tensorboard\n",
    "\n",
    "\n",
    "#=========\n",
    "#log all hyperparameters as text in Tensorboard\n",
    "#form a dictionary to with all hyperparameters to be logged\n",
    "hparam_dict = {\"train_checkpoint_key\": checkpoint_key,\n",
    "               \"batch_size\":str(batch_size),\n",
    "                \"final_activation\":final_activation,\n",
    "                \"depth\":str(depth),\n",
    "                \"num_fmaps\":str(num_fmaps),\n",
    "                \"fmap_inc_factor\":str(fmap_inc_factor),\n",
    "                \"downsample_factor\":str(downsample_factor),\n",
    "                \"kernel_size\":str(kernel_size),\n",
    "                \"padding\":padding,\n",
    "                \"upsample_mode\":upsample_mode,\n",
    "                \"loss_function\":str(loss_function),\n",
    "                \"bin_threshold\":str(bin_threshold),\n",
    "                \"optimizer\":str(optimizer),\n",
    "                \"metric\":str(metric),\n",
    "                \"n_epochs\":str(n_epochs)}\n",
    "\n",
    "#transform the dictionary in a table-like string object\n",
    "hparam_table_like = dict2mdtable(hparam_dict, key='Name', val='Value', transform_2_string=False)\n",
    "\n",
    "#log the text in Tensorboard summary of the run\n",
    "logger.add_text('Hyperparams', hparam_table_like, 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip2_segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
